{
  "project": {
    "name": "AlphaZero Light - Connect Four (4-in-a-Row)",
    "version": "2.0",
    "branch": "training-light-2",
    "description": "A lightweight implementation of the AlphaZero reinforcement learning algorithm specifically optimized for training Connect Four (4-in-a-row) game AI. This is a clean, focused version containing only essential components for training without debugging artifacts or unrelated game implementations.",
    "created": "2025-12-23",
    "repository": "https://github.com/michael-w271/alphazerolight",
    "language": "Python 3.10",
    "framework": "PyTorch"
  },
  
  "overview": {
    "purpose": "Train a neural network to play Connect Four at a high level using self-play reinforcement learning without human game data",
    "algorithm": "AlphaZero - combines deep neural networks with Monte Carlo Tree Search (MCTS) for game playing",
    "training_method": "Self-play with progressive difficulty warmup to bootstrap learning from scratch",
    "target_game": "Connect Four (6 rows × 7 columns, 4-in-a-row to win)",
    "key_features": [
      "Pure self-play learning (no human data required)",
      "Progressive opponent difficulty during training warmup",
      "Real-time training monitoring and model evaluation",
      "GPU-accelerated training with CUDA support",
      "Automated checkpoint saving and model versioning",
      "Built-in tactical puzzles for model testing"
    ]
  },

  "architecture": {
    "neural_network": {
      "type": "ResNet (Residual Network)",
      "input": "6×7 board state with 3 planes (player pieces, opponent pieces, valid moves)",
      "output": {
        "policy_head": "Probability distribution over 7 columns (move selection)",
        "value_head": "Single value estimating win probability from current position"
      },
      "configuration": {
        "num_res_blocks": 9,
        "num_hidden_channels": 128,
        "activation": "ReLU",
        "normalization": "Batch Normalization"
      }
    },
    "mcts": {
      "description": "Monte Carlo Tree Search guides exploration during self-play",
      "num_searches": 150,
      "exploration_constant": 2.0,
      "temperature": 1.3,
      "dirichlet_alpha": 0.5,
      "dirichlet_epsilon": 0.25,
      "purpose": "Balances exploitation of known good moves with exploration of uncertain positions"
    },
    "training_loop": {
      "iterations": 50,
      "self_play_games_per_iteration": 150,
      "training_epochs_per_iteration": 5,
      "batch_size": 128,
      "optimizer": "Adam",
      "learning_rate": 0.002,
      "weight_decay": 0.0001
    }
  },

  "key_innovations": {
    "progressive_warmup": {
      "description": "Gradual opponent difficulty increase prevents catastrophic forgetting",
      "phases": [
        "Iterations 1-15: 100% random opponents (pure exploration)",
        "Iterations 16-30: 75% random, 25% self-play (introducing strategy)",
        "Iterations 31+: 25% random, 75% self-play (competitive training)"
      ],
      "rationale": "Bootstraps learning from scratch without external data by ensuring the model learns basic tactics before facing strong opponents"
    },
    "value_loss_weighting": {
      "description": "Increased weight on value head to improve position evaluation",
      "policy_loss_weight": 0.5,
      "value_loss_weight": 2.0,
      "purpose": "Helps model learn to recognize winning/losing positions early in training"
    },
    "tactical_evaluation": {
      "description": "Test suite of specific game scenarios to measure learning progress",
      "tests": [
        "Blocking immediate opponent threats (3-in-a-row)",
        "Recognizing winning moves (own 3-in-a-row)",
        "Avoiding trap positions",
        "Managing endgame scenarios"
      ]
    }
  },

  "directory_structure": {
    "root_files": {
      "start_training_monitored.sh": "Main entry point - launches training with live monitoring in separate terminals",
      "run_training.sh": "Simple training launcher without monitoring",
      "env_config.sh": "Environment configuration (conda activation, Python paths, PyTorch setup)",
      "requirements.txt": "Python dependencies",
      "setup.py": "Package installation configuration",
      "README.md": "Project documentation"
    },
    "scripts": {
      "train_connect4.py": "Main training script - orchestrates self-play, MCTS, and neural network training",
      "test_model_progress.py": "Automated model evaluation - tests tactical abilities after each iteration",
      "visualize_training.py": "Plots training metrics (loss curves, win rates) from training history"
    },
    "src/alpha_zero_light": {
      "game": {
        "game.py": "Abstract base class defining game interface",
        "connect_four.py": "Complete Connect Four game logic (state, moves, win detection)"
      },
      "model": {
        "network.py": "ResNet neural network implementation (policy + value heads)"
      },
      "mcts": {
        "mcts.py": "Monte Carlo Tree Search implementation with neural network guidance"
      },
      "training": {
        "trainer.py": "AlphaZeroTrainer class - manages entire training loop",
        "evaluator.py": "Model evaluation against various opponents",
        "heuristic_opponent.py": "Rule-based opponents for training and testing",
        "tactical_trainer.py": "Tactical puzzle definitions and evaluation"
      },
      "config_connect4.py": "All hyperparameters and training configuration"
    },
    "docs": {
      "TRAINING_DOCUMENTATION.md": "Comprehensive training methodology guide",
      "training_plots": "Visualizations of training progress"
    }
  },

  "training_pipeline": {
    "phase_1_initialization": {
      "step": 1,
      "action": "Create fresh neural network with random weights",
      "output": "Untrained model ready for iteration 0"
    },
    "phase_2_self_play": {
      "step": 2,
      "action": "Generate training data through self-play games",
      "details": [
        "Current model plays against itself (or random opponent during warmup)",
        "MCTS guides move selection to create high-quality training examples",
        "Each game produces state-action-value tuples",
        "Progressive opponent mixing adjusts difficulty"
      ],
      "output": "150 games × ~15 moves = ~2,250 training examples per iteration"
    },
    "phase_3_training": {
      "step": 3,
      "action": "Train neural network on self-play data",
      "details": [
        "Policy head learns to predict MCTS-improved move probabilities",
        "Value head learns to predict game outcomes from positions",
        "5 epochs through all collected data",
        "Batch size 128 for stable gradient updates"
      ],
      "output": "Updated model weights saved to checkpoint"
    },
    "phase_4_evaluation": {
      "step": 4,
      "action": "Test model on tactical puzzles and against benchmarks",
      "tests": [
        "Block immediate threats",
        "Recognize winning moves",
        "Play against random opponent (should win >95%)",
        "Play against heuristic opponent"
      ],
      "output": "Evaluation metrics logged to model_test_results.json"
    },
    "phase_5_iteration": {
      "step": 5,
      "action": "Repeat phases 2-4 for configured number of iterations",
      "checkpoint_frequency": "Every iteration",
      "total_iterations": 50,
      "estimated_time": "~2-4 hours on GPU, ~12-24 hours on CPU"
    }
  },

  "usage": {
    "quick_start": {
      "prerequisites": [
        "Linux system (or WSL on Windows)",
        "Miniconda/Miniforge installed at /mnt/ssd2pro/miniforge3",
        "CUDA-capable GPU (optional but recommended)",
        "Python 3.10 environment named 'azl'"
      ],
      "installation": [
        "git clone https://github.com/michael-w271/alphazerolight.git",
        "cd alphazerolight",
        "git checkout training-light-2",
        "conda create -n azl python=3.10",
        "conda activate azl",
        "pip install -r requirements.txt"
      ],
      "training": [
        "Update CONDA_ROOT in env_config.sh if needed",
        "bash start_training_monitored.sh",
        "Monitor training in opened terminal windows",
        "Stop training: pkill -f train_connect4"
      ]
    },
    "configuration": {
      "file": "src/alpha_zero_light/config_connect4.py",
      "modifiable_parameters": {
        "num_iterations": "Total training iterations (default: 50)",
        "num_self_play_iterations": "Games per iteration (default: 150)",
        "num_searches": "MCTS simulations per move (default: 150)",
        "num_epochs": "Training epochs per iteration (default: 5)",
        "batch_size": "Mini-batch size (default: 128)",
        "learning_rate": "Adam optimizer learning rate (default: 0.002)",
        "opponent_mix": "Random vs self-play ratios by iteration phase"
      }
    },
    "output_files": {
      "checkpoints/connect4/model_X.pt": "Neural network weights after iteration X",
      "checkpoints/connect4/training_history.json": "Complete training metrics log",
      "model_test_results.json": "Tactical evaluation results over time",
      "training_log.txt": "Detailed console output from training process"
    }
  },

  "technical_details": {
    "game_representation": {
      "board_encoding": "3D tensor (3 × 6 × 7)",
      "channel_0": "Current player's pieces (1 where player has pieces, 0 elsewhere)",
      "channel_1": "Opponent's pieces (1 where opponent has pieces, 0 elsewhere)",
      "channel_2": "Valid move mask (1 for playable columns, 0 for full columns)",
      "coordinate_system": "Row 0 = bottom, Column 0 = leftmost",
      "player_encoding": "Player 1 = +1, Player -1 = -1 (alternating turns)"
    },
    "mcts_algorithm": {
      "selection": "UCB1 formula with neural network prior and exploration bonus",
      "expansion": "Add new node to tree on first visit",
      "simulation": "Use neural network value head (no rollout)",
      "backpropagation": "Update visit counts and average values up the tree",
      "root_noise": "Dirichlet noise added to root for exploration",
      "temperature_annealing": "High temperature early in game, low temperature in endgame"
    },
    "loss_functions": {
      "policy_loss": "Cross-entropy between MCTS move probabilities and network policy output",
      "value_loss": "Mean squared error between game outcome and network value prediction",
      "total_loss": "0.5 × policy_loss + 2.0 × value_loss (weighted sum)"
    },
    "performance_optimizations": {
      "gpu_acceleration": "All neural network operations on CUDA if available",
      "batch_inference": "Process multiple MCTS leaf nodes simultaneously",
      "numpy_operations": "Fast board state manipulation",
      "efficient_encoding": "Minimal tensor copies during game play"
    }
  },

  "debugging_and_resolved_issues": {
    "historical_problems": [
      {
        "issue": "Model failed to learn basic tactics (couldn't block threats)",
        "cause": "Pure self-play from random initialization creates degenerate strategies",
        "solution": "Progressive opponent warmup with high random play percentage initially"
      },
      {
        "issue": "Training loss plateaued early without meaningful improvement",
        "cause": "Insufficient focus on position evaluation (value head)",
        "solution": "Increased value loss weight from 1.0 to 2.0"
      },
      {
        "issue": "Terminal state bug - model sometimes played after game ended",
        "cause": "MCTS not properly detecting terminal states",
        "solution": "Fixed terminal state checking in MCTS before expansion"
      },
      {
        "issue": "Perspective encoding bug - model confused whose turn it was",
        "cause": "Board encoding didn't properly alternate player perspectives",
        "solution": "Ensured board always encoded from current player's viewpoint"
      }
    ]
  },

  "future_enhancements": {
    "planned_features": [
      "Distributed training across multiple machines",
      "Real-time visualization dashboard",
      "Human vs AI play interface",
      "Tournament mode for comparing model versions",
      "Larger board sizes (e.g., 7×7, 8×8)",
      "Different win conditions (5-in-a-row, 6-in-a-row)",
      "Opening book integration",
      "Model pruning for faster inference"
    ],
    "research_directions": [
      "Transfer learning to other connection games",
      "Curriculum learning with varied board sizes",
      "Adversarial training techniques",
      "Architecture search for optimal network design",
      "Sample efficiency improvements"
    ]
  },

  "comparison_to_original_alphazero": {
    "similarities": [
      "Self-play reinforcement learning",
      "MCTS guided by neural network policy and value",
      "ResNet-based architecture",
      "No human game data required"
    ],
    "differences": [
      "Smaller network (9 blocks vs 20-40 in full AlphaZero)",
      "Progressive warmup instead of pure self-play from start",
      "Single game focus (Connect Four) vs multi-game generality",
      "Simpler training infrastructure (single machine vs distributed)",
      "Faster training time (~hours vs days)"
    ]
  },

  "performance_metrics": {
    "expected_results": {
      "iteration_10": "Learns to block immediate threats ~60-70% of time",
      "iteration_20": "Blocks threats ~85-90%, recognizes winning moves ~70%",
      "iteration_30": "Blocks threats >95%, finds wins >90%, plays tactically",
      "iteration_50": "Strong tactical play, >98% win rate vs random, competitive vs heuristic"
    },
    "training_speed": {
      "gpu_rtx3090": "~3 minutes per iteration, ~2.5 hours total",
      "gpu_gtx1080": "~6 minutes per iteration, ~5 hours total",
      "cpu_modern": "~30 minutes per iteration, ~25 hours total"
    }
  },

  "lessons_learned": {
    "critical_insights": [
      "Pure self-play from random initialization fails without careful bootstrapping",
      "Progressive difficulty crucial for avoiding local optima",
      "Value head weight significantly impacts learning dynamics",
      "Tactical tests provide earlier signal than pure win rate",
      "MCTS quality (num_searches) trades off with training time but improves data quality",
      "Balanced exploration vs exploitation essential throughout training"
    ]
  },

  "dependencies": {
    "core": {
      "torch": ">=2.0.0 - Neural network framework",
      "numpy": ">=1.24.0 - Numerical operations",
      "tqdm": "Progress bars for training loops"
    },
    "optional": {
      "matplotlib": "Training visualization",
      "scipy": "Statistical analysis"
    },
    "system": {
      "cuda": ">=11.8 (optional, for GPU acceleration)",
      "python": "3.10 - Primary development version"
    }
  },

  "citations_and_references": {
    "papers": [
      "Silver, D., et al. (2017). Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. arXiv:1712.01815",
      "Silver, D., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489"
    ],
    "related_projects": [
      "AlphaZero-General: https://github.com/suragnair/alpha-zero-general",
      "LeelaChessZero: https://github.com/LeelaChessZero/lc0",
      "KataGo: https://github.com/lightvector/KataGo"
    ]
  },

  "license": "MIT",
  
  "contact": {
    "repository": "https://github.com/michael-w271/alphazerolight",
    "branch": "training-light-2",
    "issues": "https://github.com/michael-w271/alphazerolight/issues"
  },

  "changelog": {
    "training-light-2": {
      "date": "2025-12-23",
      "changes": [
        "Removed all debugging scripts and test files from root directory",
        "Removed website dashboard components",
        "Removed Gomoku and TicTacToe implementations (Connect Four focus only)",
        "Removed C++ MCTS experimental code",
        "Fixed conda environment paths in configuration",
        "Fixed import errors (removed TicTacToe from game module)",
        "Cleaned up documentation (removed bug reports, kept training guides)",
        "Added Snake-neural-network to gitignore",
        "Reduced repository size by ~70% while keeping all essential training code",
        "Streamlined to core components: training, evaluation, visualization"
      ],
      "migration_from": "4inarow branch",
      "improvements": "Faster setup, clearer structure, easier to understand and modify"
    }
  }
}
