{
  "issue_summary": {
    "title": "AlphaZero Connect4 Model Not Learning Tactics After 200 Iterations",
    "severity": "CRITICAL",
    "date": "2025-12-15",
    "status": "Under Investigation"
  },

  "symptoms": {
    "win_rate": {
      "expected": "Should improve from ~50% to 80%+ by iteration 100",
      "actual": "Stayed flat at 39-49% through all 200 iterations",
      "conclusion": "Model learned NOTHING beyond random play"
    },
    "loss_values": {
      "expected": "Should decrease from ~2.1 to <1.5 by iteration 100",
      "actual": "Stuck at 2.1-2.15 throughout training",
      "policy_loss": "~1.78-1.83 (not decreasing)",
      "value_loss": "~0.16-0.18 (not decreasing)"
    },
    "tactical_tests": {
      "find_immediate_win": "8.7% confidence (random = 14.3%) - WORSE than random!",
      "block_opponent_threat": "8.7-26% confidence (need >30%)",
      "empty_board_value": "PASS - correctly evaluates as ~0.0",
      "overall": "1/4 tests passing - only recognizes empty board"
    },
    "sign_verification": {
      "player_1_winning_positions": "CORRECT - positive values (+0.43 to +0.85)",
      "player_minus1_winning_positions": "CORRECT - negative values (-0.81 to -0.99)",
      "empty_board": "WRONG - biased negative (-0.83, should be ~0.0)",
      "conclusion": "Signs are mostly correct, so encoding fix worked, but learning is broken"
    }
  },

  "bugs_fixed_so_far": [
    {
      "bug": "Terminal state checked AFTER perspective flip in MCTS",
      "fix": "Check terminal BEFORE flip, store with node",
      "date": "2025-12-14",
      "file": "src/alpha_zero_light/mcts/mcts.py",
      "lines": "64-77"
    },
    {
      "bug": "States encoded without converting to player perspective",
      "fix": "Call change_perspective(hist_state, hist_player) before encoding",
      "date": "2025-12-15",
      "file": "src/alpha_zero_light/training/trainer.py",
      "lines": "102-104, 150-152",
      "impact": "CRITICAL - This was causing Player -1's pieces to be labeled as opponent"
    }
  ],

  "remaining_mystery": {
    "question": "Why is the model not learning tactics despite correct signs?",
    "observations": [
      "Model CAN recognize terminal positions (strong negative for opponent wins)",
      "Model CANNOT learn one-move-ahead tactics (find wins, block threats)",
      "Empty board bias suggests systematic issue in training data",
      "Loss not decreasing = model not finding patterns in data"
    ],
    "hypotheses": [
      "Perspective conversion still wrong somewhere",
      "MCTS backpropagation has sign error",
      "Training data collection corrupted",
      "Policy and value heads learning conflicting information"
    ]
  },

  "critical_code_sections": {
    "trainer_outcome_assignment": {
      "file": "src/alpha_zero_light/training/trainer.py",
      "lines": "98-107 and 146-155",
      "description": "Assigns +1/-1 outcomes to positions after game ends",
      "current_logic": "hist_outcome = value if hist_player == player else -value",
      "questions_to_verify": [
        "Is 'player' the player who JUST moved (winner)?",
        "Is 'hist_player' the player whose move we're labeling?",
        "Is 'value' always 1 when someone wins?",
        "After change_perspective, does state have hist_player's pieces as +1?"
      ]
    },
    
    "mcts_terminal_handling": {
      "file": "src/alpha_zero_light/mcts/mcts.py",
      "lines": "64-77",
      "description": "Detects terminal states and flips value for child perspective",
      "current_logic": "Check terminal BEFORE flip, then terminal_value = -terminal_value for child",
      "questions_to_verify": [
        "Is terminal_value from parent's or child's perspective?",
        "Should we negate it for the child?",
        "Is backpropagation flipping signs correctly?"
      ]
    },

    "mcts_backpropagation": {
      "file": "src/alpha_zero_light/mcts/mcts.py",
      "lines": "79-85",
      "description": "Propagates values up the tree",
      "current_logic": "value = -value, then parent.backpropagate(value)",
      "questions_to_verify": [
        "Should value be negated going up the tree?",
        "Does this correctly alternate between players?",
        "Is value_sum accumulated correctly?"
      ]
    },

    "state_encoding": {
      "file": "src/alpha_zero_light/game/connect_four.py",
      "lines": "198-226",
      "description": "Encodes board as 3 channels: [opponent, empty, current_player]",
      "questions_to_verify": [
        "Channel 0: state == -1 (opponent pieces) - is this from current player's view?",
        "Channel 1: state == 0 (empty cells)",
        "Channel 2: state == 1 (current player pieces)",
        "Does model see +1 values as 'me' or 'Player 1'?"
      ]
    }
  },

  "diagnostic_steps": [
    {
      "step": 1,
      "action": "Trace one complete self-play game",
      "what_to_log": [
        "Each move: which player, what state perspective, what action",
        "Game end: who won, value returned, player variable",
        "For each historical position: player, perspective before/after change, outcome assigned",
        "Encoding: what goes into each channel"
      ]
    },
    {
      "step": 2,
      "action": "Verify MCTS terminal value propagation",
      "what_to_log": [
        "Parent makes move leading to terminal state",
        "What value does child node store?",
        "What value gets backpropagated to parent?",
        "Does UCB calculation use correct sign?"
      ]
    },
    {
      "step": 3,
      "action": "Check training data statistics",
      "what_to_log": [
        "Distribution of outcomes: how many +1, how many -1?",
        "For winning positions: are values positive?",
        "For losing positions: are values negative?",
        "Do encoded states match the outcome labels?"
      ]
    }
  ],

  "next_actions": [
    "Review trainer.py lines 85-157 for any missed perspective bugs",
    "Review mcts.py lines 48-85 for backpropagation sign errors",
    "Add extensive debug logging to trace one game end-to-end",
    "Verify training data is correctly labeled before training",
    "Consider if MCTS is even searching properly (could UCB be broken?)"
  ],

  "files_to_investigate": [
    {
      "priority": 1,
      "file": "src/alpha_zero_light/training/trainer.py",
      "focus_lines": "85-157",
      "what_to_check": "Outcome assignment and perspective conversion"
    },
    {
      "priority": 1,
      "file": "src/alpha_zero_light/mcts/mcts.py",
      "focus_lines": "48-85, 192-220",
      "what_to_check": "Terminal value handling and backpropagation"
    },
    {
      "priority": 2,
      "file": "src/alpha_zero_light/game/connect_four.py",
      "focus_lines": "171-185, 198-226",
      "what_to_check": "What perspective does get_value_and_terminated use? Encoding correctness"
    }
  ],

  "evidence_the_fix_worked": [
    "Win rate did NOT collapse to 1% (like before)",
    "Sign verification shows correct polarity (+ for Player 1 winning)",
    "Model strongly recognizes terminal positions (-0.99 for opponent wins)"
  ],

  "evidence_something_still_wrong": [
    "No improvement over 200 iterations",
    "Cannot detect one-move tactics",
    "Loss completely flat",
    "Empty board bias (-0.83 instead of 0.0)",
    "Win detection WORSE than random (8.7% vs 14.3%)"
  ],

  "most_likely_culprit": "There's still a subtle perspective or sign bug, possibly in MCTS backpropagation or in how we're interpreting the return value from get_value_and_terminated when playing against opponents. The fact that empty board is biased negative suggests systematic labeling error."
}
